# Intelligent Data Analysis Platform

An advanced, **AI-driven** platform for automated data analysis.  
It features a modular architecture that intelligently processes user queries for both **web-based data scraping** and **complex database analysis**.

The platform dynamically generates and executes Python code on the fly using a **Large Language Model (LLM)**.  
It includes a self-healing mechanism to debug and retry failed code executions, making it highly resilient for real-world tasks.

---

## ðŸš€ Features

- **Dual-Workflow Architecture** â€” Handles both web scraping and database analysis tasks.
- **Dynamic Code Generation** â€” Creates tailored Python scripts per query instead of relying on fixed logic.
- **Self-Healing Execution** â€” Detects errors, requests LLM-based fixes, and retries automatically.
- **Robust Web Scraping** â€” Uses Playwright with stealth mode for JavaScript-heavy sites.
- **Intelligent Data Cleaning** â€” LLM-powered numeric data detection and formatting.
- **Extensible & Modular** â€” Easy to add new workflows or integrations.

---

## ðŸ“‚ Project Structure

reshavs project/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ main.py # FastAPI app entry point
â”‚ â””â”€â”€ api.py # Core API logic
â”œâ”€â”€ core/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ base.py # Workflow base classes
â”‚ â””â”€â”€ config.py # Config & LLM setup
â”œâ”€â”€ workflows/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ web_scraping.py # Web scraping workflow
â”‚ â””â”€â”€ database_analysis.py # Database workflow
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ constants.py # Project constants
â”‚ â”œâ”€â”€ duckdb_utils.py # DuckDB helpers
â”‚ â””â”€â”€ prompts.py # LLM prompts
â”œâ”€â”€ tests/
â”‚ â””â”€â”€ test_api.py # API tests
â”œâ”€â”€ .env.example # Example env vars
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt # Python deps
â””â”€â”€ README.md

yaml
Copy
Edit

---

## âš™ How It Works

### 1. **Web Scraping Workflow** (`multi_step_web_scraping`)
Triggered when a query contains a **URL**.

1. **Fetch Data** â€” Playwright scrapes full HTML.
2. **Extract & Clean** â€” Main table â†’ pandas DataFrame â†’ cleaned.
3. **Save CSV** â€” Stored as `temp_web_data.csv`.
4. **Generate Python Script** â€” LLM writes a pandas script for analysis.
5. **Execute & Self-Fix** â€” Runs script, retries on failure.
6. **Return Result** â€” Outputs JSON.

---

### 2. **Database Analysis Workflow** (`database_analysis`)
Triggered when the query references a **database** (e.g., S3 path).

1. **Create Data Summary** â€” Extracts schema & details.
2. **Generate Python Script** â€” LLM writes a DuckDB script.
3. **Execute & Self-Fix** â€” Runs script, retries on failure.
4. **Return Result** â€” Outputs JSON.

---

## ðŸ›  Setup & Installation

### Prerequisites
- Python 3.10+
- Google Gemini API Key

### 1. Clone the Repository
```bash
git clone <your-repository-url>
cd "reshavs project"
2. Create a Virtual Environment
bash
Copy
Edit
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
3. Install Dependencies
bash
Copy
Edit
pip install -r requirements.txt
4. Configure Environment Variables
Rename .env.example â†’ .env and set your key:

env
Copy
Edit
GEMINI_API_KEY="your_actual_gemini_api_key_here"
5. Install Playwright Browsers
bash
Copy
Edit
playwright install
â–¶ Running the Application
bash
Copy
Edit
uvicorn app.main:app --reload
API will be available at:
http://127.0.0.1:8000

ðŸ“¡ API Usage
Endpoint
bash
Copy
Edit
POST /api/
Form Data:

questions_txt â†’ Path to .txt file containing your query.

Example â€” Web Scraping
File: wiki_films_question.txt

plaintext
Copy
Edit
Scrape the list of highest grossing films from Wikipedia...
URL: https://en.wikipedia.org/wiki/List_of_highest-grossing_films

1. How many $2 bn movies were released before 2000?
2. Which is the earliest film that grossed over $1.5 bn?
3. What's the correlation between Rank and Peak?
4. Draw a scatterplot of Rank and Peak with a red dotted regression line.
Run:

bash
Copy
Edit
curl -X POST "http://127.0.0.1:8000/api/" \
     -F "questions_txt=@wiki_films_question.txt"
Example â€” Database Analysis
File: high_court_question.txt

plaintext
Copy
Edit
The Indian high court judgement dataset is located at:
s3://indian-high-court-judgments/...

Q1. Which high court disposed the most cases from 2019 - 2022?
Q2. Regression slope of date_of_registration - decision_date by year for court=33_10?
Q3. Scatterplot of year vs. delay days with regression line.
Run:

bash
Copy
Edit
curl -X POST "http://127.0.0.1:8000/api/" \
     -F "questions_txt=@high_court_question.txt"